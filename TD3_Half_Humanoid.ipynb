{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_Half_Humanoid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "670ebbc8-ae0f-4676-a60a-4077d45d3167"
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/2e/558ec393fb7914662f0fa5cd7adcb49c9009927fb5395d8800cb5bdafaad/pybullet-2.7.7.tar.gz (83.7MB)\n",
            "\u001b[K     |████████████████████████████████| 83.7MB 55kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybullet: filename=pybullet-2.7.7-cp36-cp36m-linux_x86_64.whl size=95743278 sha256=0502f41059f3ed5838983988cc26190e32fd3bea894ebe6e64a8ef285e9cfb42\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/10/be/0e11971182fb75c5a2f0e00491f35a9aa31bb5824cc7aea6c6\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "    \n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH0trzYHieIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_21 = nn.Linear(300, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = F.relu(self.layer_21(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_21 = nn.Linear(300, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_51 = nn.Linear(300, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = F.relu(self.layer_21(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = F.relu(self.layer_51(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = F.relu(self.layer_21(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"Walker2DBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 3e4 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5c18a6d1-4233-4c18-9ba4-3e0829f8a745"
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_Walker2DBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3a79aac5-1672-4976-b6ce-77ed6ff3d0c7"
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e3b0173c-068b-4630-c0e8-4eeb6ba4a2d7"
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 185.693578\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf830f72-2b1a-48cd-acd5-d4d4f8f45de9"
      },
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  reward = []\n",
        "  time_stamp = []\n",
        "  step = 0\n",
        "\n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      reward.append(evaluate_policy(policy))\n",
        "      step = step+1\n",
        "      time_stamp.append(step)\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "\n",
        "\n",
        "plt.plot(time_stamp, reward)\n",
        "\n",
        "plt.xlabel('time stamp')\n",
        "plt.ylabel('reward')\n",
        "\n",
        "plt.title('Line graph!')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 15 Episode Num: 1 Reward: 15.621491387415155\n",
            "Total Timesteps: 28 Episode Num: 2 Reward: 15.447537327323518\n",
            "Total Timesteps: 37 Episode Num: 3 Reward: 14.912599736674746\n",
            "Total Timesteps: 55 Episode Num: 4 Reward: 18.118876980076312\n",
            "Total Timesteps: 70 Episode Num: 5 Reward: 16.518420667249302\n",
            "Total Timesteps: 89 Episode Num: 6 Reward: 19.697330732837145\n",
            "Total Timesteps: 99 Episode Num: 7 Reward: 15.161703841165581\n",
            "Total Timesteps: 110 Episode Num: 8 Reward: 15.146947394919698\n",
            "Total Timesteps: 119 Episode Num: 9 Reward: 14.629616305934904\n",
            "Total Timesteps: 129 Episode Num: 10 Reward: 14.70163895840669\n",
            "Total Timesteps: 146 Episode Num: 11 Reward: 17.571659284432823\n",
            "Total Timesteps: 154 Episode Num: 12 Reward: 13.917644853310776\n",
            "Total Timesteps: 175 Episode Num: 13 Reward: 17.71147210028867\n",
            "Total Timesteps: 185 Episode Num: 14 Reward: 15.306307065914734\n",
            "Total Timesteps: 202 Episode Num: 15 Reward: 18.280459924548627\n",
            "Total Timesteps: 215 Episode Num: 16 Reward: 16.814963859142154\n",
            "Total Timesteps: 232 Episode Num: 17 Reward: 16.2303334622382\n",
            "Total Timesteps: 256 Episode Num: 18 Reward: 23.511772180932173\n",
            "Total Timesteps: 267 Episode Num: 19 Reward: 14.67337873276556\n",
            "Total Timesteps: 278 Episode Num: 20 Reward: 13.163840815379807\n",
            "Total Timesteps: 296 Episode Num: 21 Reward: 19.759605988474508\n",
            "Total Timesteps: 306 Episode Num: 22 Reward: 14.78797265268513\n",
            "Total Timesteps: 315 Episode Num: 23 Reward: 12.926264585719037\n",
            "Total Timesteps: 331 Episode Num: 24 Reward: 17.931033349430074\n",
            "Total Timesteps: 354 Episode Num: 25 Reward: 23.901795531320392\n",
            "Total Timesteps: 370 Episode Num: 26 Reward: 17.007741266729134\n",
            "Total Timesteps: 387 Episode Num: 27 Reward: 17.93964700844517\n",
            "Total Timesteps: 395 Episode Num: 28 Reward: 13.4145367458259\n",
            "Total Timesteps: 408 Episode Num: 29 Reward: 14.731759888165106\n",
            "Total Timesteps: 425 Episode Num: 30 Reward: 19.077657784101028\n",
            "Total Timesteps: 434 Episode Num: 31 Reward: 13.739125609400798\n",
            "Total Timesteps: 452 Episode Num: 32 Reward: 19.611618212297614\n",
            "Total Timesteps: 470 Episode Num: 33 Reward: 19.6463517874261\n",
            "Total Timesteps: 484 Episode Num: 34 Reward: 15.647852573807176\n",
            "Total Timesteps: 501 Episode Num: 35 Reward: 18.334651199298968\n",
            "Total Timesteps: 516 Episode Num: 36 Reward: 17.143541918958363\n",
            "Total Timesteps: 526 Episode Num: 37 Reward: 13.904933701505069\n",
            "Total Timesteps: 533 Episode Num: 38 Reward: 11.95604971845023\n",
            "Total Timesteps: 550 Episode Num: 39 Reward: 17.05978684932343\n",
            "Total Timesteps: 571 Episode Num: 40 Reward: 22.772292751572966\n",
            "Total Timesteps: 589 Episode Num: 41 Reward: 19.61448057942616\n",
            "Total Timesteps: 604 Episode Num: 42 Reward: 17.298570016771556\n",
            "Total Timesteps: 611 Episode Num: 43 Reward: 12.676536879490593\n",
            "Total Timesteps: 628 Episode Num: 44 Reward: 21.00299075794756\n",
            "Total Timesteps: 639 Episode Num: 45 Reward: 14.635017632762901\n",
            "Total Timesteps: 647 Episode Num: 46 Reward: 12.658642018798854\n",
            "Total Timesteps: 659 Episode Num: 47 Reward: 14.088932728089276\n",
            "Total Timesteps: 672 Episode Num: 48 Reward: 16.357897707703522\n",
            "Total Timesteps: 691 Episode Num: 49 Reward: 20.415412529358584\n",
            "Total Timesteps: 703 Episode Num: 50 Reward: 14.019409631272719\n",
            "Total Timesteps: 713 Episode Num: 51 Reward: 13.304468175928921\n",
            "Total Timesteps: 724 Episode Num: 52 Reward: 16.90668976169982\n",
            "Total Timesteps: 743 Episode Num: 53 Reward: 18.802108528434474\n",
            "Total Timesteps: 754 Episode Num: 54 Reward: 17.139318583846034\n",
            "Total Timesteps: 766 Episode Num: 55 Reward: 16.34387399310217\n",
            "Total Timesteps: 772 Episode Num: 56 Reward: 10.976422345233733\n",
            "Total Timesteps: 784 Episode Num: 57 Reward: 16.159535773783865\n",
            "Total Timesteps: 801 Episode Num: 58 Reward: 17.889822108291263\n",
            "Total Timesteps: 816 Episode Num: 59 Reward: 18.041842138049834\n",
            "Total Timesteps: 826 Episode Num: 60 Reward: 14.861172160522255\n",
            "Total Timesteps: 838 Episode Num: 61 Reward: 18.161813740697106\n",
            "Total Timesteps: 855 Episode Num: 62 Reward: 18.6422959869451\n",
            "Total Timesteps: 864 Episode Num: 63 Reward: 13.350884020249941\n",
            "Total Timesteps: 884 Episode Num: 64 Reward: 23.004747287475038\n",
            "Total Timesteps: 896 Episode Num: 65 Reward: 14.678992693839245\n",
            "Total Timesteps: 908 Episode Num: 66 Reward: 16.1445946609485\n",
            "Total Timesteps: 923 Episode Num: 67 Reward: 17.697805424311085\n",
            "Total Timesteps: 935 Episode Num: 68 Reward: 13.866355909594859\n",
            "Total Timesteps: 947 Episode Num: 69 Reward: 17.42237562230148\n",
            "Total Timesteps: 966 Episode Num: 70 Reward: 21.85121708317165\n",
            "Total Timesteps: 975 Episode Num: 71 Reward: 12.617081786380732\n",
            "Total Timesteps: 994 Episode Num: 72 Reward: 18.01176700599899\n",
            "Total Timesteps: 1003 Episode Num: 73 Reward: 13.162770070083209\n",
            "Total Timesteps: 1014 Episode Num: 74 Reward: 15.697276284656255\n",
            "Total Timesteps: 1032 Episode Num: 75 Reward: 17.81498660428042\n",
            "Total Timesteps: 1053 Episode Num: 76 Reward: 24.018496926636725\n",
            "Total Timesteps: 1064 Episode Num: 77 Reward: 14.672545738592452\n",
            "Total Timesteps: 1073 Episode Num: 78 Reward: 12.212769816174113\n",
            "Total Timesteps: 1102 Episode Num: 79 Reward: 24.285760479824962\n",
            "Total Timesteps: 1118 Episode Num: 80 Reward: 18.87917551954597\n",
            "Total Timesteps: 1142 Episode Num: 81 Reward: 22.789104693778793\n",
            "Total Timesteps: 1161 Episode Num: 82 Reward: 20.451607372428406\n",
            "Total Timesteps: 1185 Episode Num: 83 Reward: 17.59965646757482\n",
            "Total Timesteps: 1200 Episode Num: 84 Reward: 16.093303014783306\n",
            "Total Timesteps: 1211 Episode Num: 85 Reward: 14.59437546888803\n",
            "Total Timesteps: 1226 Episode Num: 86 Reward: 18.11808394529362\n",
            "Total Timesteps: 1234 Episode Num: 87 Reward: 12.038963874379988\n",
            "Total Timesteps: 1250 Episode Num: 88 Reward: 17.299127759544355\n",
            "Total Timesteps: 1259 Episode Num: 89 Reward: 14.06089317865699\n",
            "Total Timesteps: 1274 Episode Num: 90 Reward: 19.38749890958424\n",
            "Total Timesteps: 1288 Episode Num: 91 Reward: 16.22377954093972\n",
            "Total Timesteps: 1302 Episode Num: 92 Reward: 16.437582985695915\n",
            "Total Timesteps: 1315 Episode Num: 93 Reward: 17.61527211524226\n",
            "Total Timesteps: 1334 Episode Num: 94 Reward: 22.24278722687304\n",
            "Total Timesteps: 1361 Episode Num: 95 Reward: 26.79917088142247\n",
            "Total Timesteps: 1375 Episode Num: 96 Reward: 16.612229664361806\n",
            "Total Timesteps: 1384 Episode Num: 97 Reward: 14.62299566500733\n",
            "Total Timesteps: 1394 Episode Num: 98 Reward: 13.760005843665567\n",
            "Total Timesteps: 1409 Episode Num: 99 Reward: 18.9380096667679\n",
            "Total Timesteps: 1426 Episode Num: 100 Reward: 19.335386866344198\n",
            "Total Timesteps: 1443 Episode Num: 101 Reward: 19.239044615393507\n",
            "Total Timesteps: 1455 Episode Num: 102 Reward: 15.768328419559111\n",
            "Total Timesteps: 1468 Episode Num: 103 Reward: 15.364396697230406\n",
            "Total Timesteps: 1486 Episode Num: 104 Reward: 16.413724472894682\n",
            "Total Timesteps: 1496 Episode Num: 105 Reward: 14.670622165295937\n",
            "Total Timesteps: 1508 Episode Num: 106 Reward: 16.385623635951198\n",
            "Total Timesteps: 1519 Episode Num: 107 Reward: 15.384158548891719\n",
            "Total Timesteps: 1541 Episode Num: 108 Reward: 21.137620462941413\n",
            "Total Timesteps: 1560 Episode Num: 109 Reward: 21.415477581376035\n",
            "Total Timesteps: 1596 Episode Num: 110 Reward: 23.66113883017097\n",
            "Total Timesteps: 1604 Episode Num: 111 Reward: 12.30436817032023\n",
            "Total Timesteps: 1616 Episode Num: 112 Reward: 15.05233161510696\n",
            "Total Timesteps: 1631 Episode Num: 113 Reward: 17.156746250885774\n",
            "Total Timesteps: 1674 Episode Num: 114 Reward: 44.43027096076112\n",
            "Total Timesteps: 1690 Episode Num: 115 Reward: 16.284998469467972\n",
            "Total Timesteps: 1702 Episode Num: 116 Reward: 13.878719771452598\n",
            "Total Timesteps: 1718 Episode Num: 117 Reward: 18.783131539303575\n",
            "Total Timesteps: 1725 Episode Num: 118 Reward: 12.43554903207696\n",
            "Total Timesteps: 1739 Episode Num: 119 Reward: 13.461563834376285\n",
            "Total Timesteps: 1750 Episode Num: 120 Reward: 15.733908293356942\n",
            "Total Timesteps: 1762 Episode Num: 121 Reward: 14.045964762990478\n",
            "Total Timesteps: 1782 Episode Num: 122 Reward: 19.192347503747435\n",
            "Total Timesteps: 1799 Episode Num: 123 Reward: 17.80421125094581\n",
            "Total Timesteps: 1822 Episode Num: 124 Reward: 22.412665068454228\n",
            "Total Timesteps: 1842 Episode Num: 125 Reward: 21.00121580359555\n",
            "Total Timesteps: 1874 Episode Num: 126 Reward: 25.206125259617696\n",
            "Total Timesteps: 1884 Episode Num: 127 Reward: 13.839557770561079\n",
            "Total Timesteps: 1894 Episode Num: 128 Reward: 14.261818286596098\n",
            "Total Timesteps: 1910 Episode Num: 129 Reward: 17.536476445801966\n",
            "Total Timesteps: 1922 Episode Num: 130 Reward: 16.208398602656963\n",
            "Total Timesteps: 1933 Episode Num: 131 Reward: 14.711104626637823\n",
            "Total Timesteps: 1950 Episode Num: 132 Reward: 22.30073370505852\n",
            "Total Timesteps: 1974 Episode Num: 133 Reward: 25.969084495122658\n",
            "Total Timesteps: 1983 Episode Num: 134 Reward: 14.12930411679263\n",
            "Total Timesteps: 1994 Episode Num: 135 Reward: 14.581021647798481\n",
            "Total Timesteps: 2015 Episode Num: 136 Reward: 21.30130125436408\n",
            "Total Timesteps: 2030 Episode Num: 137 Reward: 14.900999758708348\n",
            "Total Timesteps: 2038 Episode Num: 138 Reward: 12.280147090958781\n",
            "Total Timesteps: 2051 Episode Num: 139 Reward: 15.488598993228516\n",
            "Total Timesteps: 2072 Episode Num: 140 Reward: 22.19294393876917\n",
            "Total Timesteps: 2093 Episode Num: 141 Reward: 22.46440477292053\n",
            "Total Timesteps: 2103 Episode Num: 142 Reward: 13.443939882112318\n",
            "Total Timesteps: 2119 Episode Num: 143 Reward: 17.046638105312013\n",
            "Total Timesteps: 2127 Episode Num: 144 Reward: 13.960305676088318\n",
            "Total Timesteps: 2141 Episode Num: 145 Reward: 17.370568454937892\n",
            "Total Timesteps: 2154 Episode Num: 146 Reward: 15.085879993879644\n",
            "Total Timesteps: 2168 Episode Num: 147 Reward: 17.587580540875205\n",
            "Total Timesteps: 2185 Episode Num: 148 Reward: 18.114970959823406\n",
            "Total Timesteps: 2201 Episode Num: 149 Reward: 18.12162162730674\n",
            "Total Timesteps: 2214 Episode Num: 150 Reward: 18.500178623253305\n",
            "Total Timesteps: 2224 Episode Num: 151 Reward: 13.154560623703581\n",
            "Total Timesteps: 2253 Episode Num: 152 Reward: 25.491212465392895\n",
            "Total Timesteps: 2274 Episode Num: 153 Reward: 22.785207645405904\n",
            "Total Timesteps: 2289 Episode Num: 154 Reward: 18.06215598842391\n",
            "Total Timesteps: 2306 Episode Num: 155 Reward: 18.792280776497496\n",
            "Total Timesteps: 2327 Episode Num: 156 Reward: 16.720570948072414\n",
            "Total Timesteps: 2338 Episode Num: 157 Reward: 15.509498154750327\n",
            "Total Timesteps: 2349 Episode Num: 158 Reward: 15.408697670875698\n",
            "Total Timesteps: 2373 Episode Num: 159 Reward: 24.090754549395932\n",
            "Total Timesteps: 2391 Episode Num: 160 Reward: 20.047641727472364\n",
            "Total Timesteps: 2401 Episode Num: 161 Reward: 15.485045507279574\n",
            "Total Timesteps: 2419 Episode Num: 162 Reward: 18.65562437599292\n",
            "Total Timesteps: 2434 Episode Num: 163 Reward: 16.610312180202165\n",
            "Total Timesteps: 2451 Episode Num: 164 Reward: 16.74657483441406\n",
            "Total Timesteps: 2475 Episode Num: 165 Reward: 20.589754710669514\n",
            "Total Timesteps: 2498 Episode Num: 166 Reward: 21.66819467884925\n",
            "Total Timesteps: 2509 Episode Num: 167 Reward: 15.628022749278172\n",
            "Total Timesteps: 2521 Episode Num: 168 Reward: 14.98409390879533\n",
            "Total Timesteps: 2537 Episode Num: 169 Reward: 16.08043292227958\n",
            "Total Timesteps: 2548 Episode Num: 170 Reward: 13.310911233442312\n",
            "Total Timesteps: 2563 Episode Num: 171 Reward: 18.148330273084866\n",
            "Total Timesteps: 2575 Episode Num: 172 Reward: 17.48772426203359\n",
            "Total Timesteps: 2589 Episode Num: 173 Reward: 16.96549034555064\n",
            "Total Timesteps: 2611 Episode Num: 174 Reward: 19.69460425323632\n",
            "Total Timesteps: 2642 Episode Num: 175 Reward: 27.10591067159694\n",
            "Total Timesteps: 2653 Episode Num: 176 Reward: 16.231812122098926\n",
            "Total Timesteps: 2672 Episode Num: 177 Reward: 17.979987523514136\n",
            "Total Timesteps: 2690 Episode Num: 178 Reward: 21.691124927897178\n",
            "Total Timesteps: 2698 Episode Num: 179 Reward: 13.298136173095555\n",
            "Total Timesteps: 2707 Episode Num: 180 Reward: 13.33270090592996\n",
            "Total Timesteps: 2724 Episode Num: 181 Reward: 19.71768062129122\n",
            "Total Timesteps: 2741 Episode Num: 182 Reward: 17.487567368484452\n",
            "Total Timesteps: 2750 Episode Num: 183 Reward: 14.932354847328677\n",
            "Total Timesteps: 2759 Episode Num: 184 Reward: 13.32158484038373\n",
            "Total Timesteps: 2774 Episode Num: 185 Reward: 18.270800153512386\n",
            "Total Timesteps: 2795 Episode Num: 186 Reward: 21.166401999321533\n",
            "Total Timesteps: 2817 Episode Num: 187 Reward: 17.739899357563992\n",
            "Total Timesteps: 2827 Episode Num: 188 Reward: 14.135862558023655\n",
            "Total Timesteps: 2836 Episode Num: 189 Reward: 14.083919976840843\n",
            "Total Timesteps: 2845 Episode Num: 190 Reward: 13.417767022273619\n",
            "Total Timesteps: 2865 Episode Num: 191 Reward: 19.350403020667724\n",
            "Total Timesteps: 2873 Episode Num: 192 Reward: 13.283043131591693\n",
            "Total Timesteps: 2882 Episode Num: 193 Reward: 12.985173654741082\n",
            "Total Timesteps: 2911 Episode Num: 194 Reward: 30.164446145453255\n",
            "Total Timesteps: 2927 Episode Num: 195 Reward: 18.470215727174946\n",
            "Total Timesteps: 2936 Episode Num: 196 Reward: 14.382288400191463\n",
            "Total Timesteps: 2949 Episode Num: 197 Reward: 16.437237022021147\n",
            "Total Timesteps: 2967 Episode Num: 198 Reward: 18.79707328765507\n",
            "Total Timesteps: 2977 Episode Num: 199 Reward: 15.04380017296644\n",
            "Total Timesteps: 2987 Episode Num: 200 Reward: 13.0319418075087\n",
            "Total Timesteps: 2997 Episode Num: 201 Reward: 13.237296665638858\n",
            "Total Timesteps: 3004 Episode Num: 202 Reward: 11.132282669660345\n",
            "Total Timesteps: 3017 Episode Num: 203 Reward: 18.171520300743577\n",
            "Total Timesteps: 3028 Episode Num: 204 Reward: 14.862694648910836\n",
            "Total Timesteps: 3036 Episode Num: 205 Reward: 12.22202586651547\n",
            "Total Timesteps: 3053 Episode Num: 206 Reward: 18.35431545927131\n",
            "Total Timesteps: 3071 Episode Num: 207 Reward: 17.677945338898283\n",
            "Total Timesteps: 3089 Episode Num: 208 Reward: 18.48427399424981\n",
            "Total Timesteps: 3098 Episode Num: 209 Reward: 12.593182825268014\n",
            "Total Timesteps: 3112 Episode Num: 210 Reward: 17.58615119106398\n",
            "Total Timesteps: 3125 Episode Num: 211 Reward: 16.296552605317263\n",
            "Total Timesteps: 3134 Episode Num: 212 Reward: 14.278025131419415\n",
            "Total Timesteps: 3147 Episode Num: 213 Reward: 16.333432652035846\n",
            "Total Timesteps: 3156 Episode Num: 214 Reward: 14.083495862934795\n",
            "Total Timesteps: 3176 Episode Num: 215 Reward: 18.350544909777813\n",
            "Total Timesteps: 3183 Episode Num: 216 Reward: 11.191782856314967\n",
            "Total Timesteps: 3200 Episode Num: 217 Reward: 20.804780740819112\n",
            "Total Timesteps: 3223 Episode Num: 218 Reward: 24.359130992014254\n",
            "Total Timesteps: 3239 Episode Num: 219 Reward: 16.821096254618897\n",
            "Total Timesteps: 3251 Episode Num: 220 Reward: 15.837772296357432\n",
            "Total Timesteps: 3270 Episode Num: 221 Reward: 19.390462191640108\n",
            "Total Timesteps: 3276 Episode Num: 222 Reward: 12.133988983623567\n",
            "Total Timesteps: 3285 Episode Num: 223 Reward: 12.94835836109414\n",
            "Total Timesteps: 3300 Episode Num: 224 Reward: 17.115304104438106\n",
            "Total Timesteps: 3325 Episode Num: 225 Reward: 25.926949058065656\n",
            "Total Timesteps: 3342 Episode Num: 226 Reward: 17.981417739194878\n",
            "Total Timesteps: 3363 Episode Num: 227 Reward: 22.099120596854483\n",
            "Total Timesteps: 3371 Episode Num: 228 Reward: 12.996750859027086\n",
            "Total Timesteps: 3383 Episode Num: 229 Reward: 16.29180507127021\n",
            "Total Timesteps: 3398 Episode Num: 230 Reward: 18.196957786181883\n",
            "Total Timesteps: 3406 Episode Num: 231 Reward: 13.244165157472887\n",
            "Total Timesteps: 3424 Episode Num: 232 Reward: 17.408548266740397\n",
            "Total Timesteps: 3433 Episode Num: 233 Reward: 14.690901808223861\n",
            "Total Timesteps: 3448 Episode Num: 234 Reward: 15.625299084935977\n",
            "Total Timesteps: 3463 Episode Num: 235 Reward: 14.175486938495306\n",
            "Total Timesteps: 3471 Episode Num: 236 Reward: 11.77231953350274\n",
            "Total Timesteps: 3485 Episode Num: 237 Reward: 18.39690559286828\n",
            "Total Timesteps: 3505 Episode Num: 238 Reward: 20.479632913487144\n",
            "Total Timesteps: 3517 Episode Num: 239 Reward: 15.399160378979285\n",
            "Total Timesteps: 3532 Episode Num: 240 Reward: 17.713836714303763\n",
            "Total Timesteps: 3545 Episode Num: 241 Reward: 15.132649388206483\n",
            "Total Timesteps: 3554 Episode Num: 242 Reward: 13.682781751714353\n",
            "Total Timesteps: 3568 Episode Num: 243 Reward: 17.856901560597183\n",
            "Total Timesteps: 3576 Episode Num: 244 Reward: 14.186482367843563\n",
            "Total Timesteps: 3591 Episode Num: 245 Reward: 18.182206418782876\n",
            "Total Timesteps: 3603 Episode Num: 246 Reward: 14.13803280958382\n",
            "Total Timesteps: 3615 Episode Num: 247 Reward: 16.383241675321187\n",
            "Total Timesteps: 3626 Episode Num: 248 Reward: 12.476880050066391\n",
            "Total Timesteps: 3641 Episode Num: 249 Reward: 17.921975004770506\n",
            "Total Timesteps: 3650 Episode Num: 250 Reward: 12.687537263637932\n",
            "Total Timesteps: 3662 Episode Num: 251 Reward: 16.9495040233669\n",
            "Total Timesteps: 3672 Episode Num: 252 Reward: 14.474010943975008\n",
            "Total Timesteps: 3679 Episode Num: 253 Reward: 12.577730418158172\n",
            "Total Timesteps: 3688 Episode Num: 254 Reward: 13.894194224930832\n",
            "Total Timesteps: 3710 Episode Num: 255 Reward: 19.08075107495242\n",
            "Total Timesteps: 3727 Episode Num: 256 Reward: 18.758279137164937\n",
            "Total Timesteps: 3747 Episode Num: 257 Reward: 20.172668015018278\n",
            "Total Timesteps: 3759 Episode Num: 258 Reward: 15.788117337870064\n",
            "Total Timesteps: 3770 Episode Num: 259 Reward: 15.171485906311135\n",
            "Total Timesteps: 3781 Episode Num: 260 Reward: 14.001931109375436\n",
            "Total Timesteps: 3793 Episode Num: 261 Reward: 15.3820178585549\n",
            "Total Timesteps: 3811 Episode Num: 262 Reward: 18.96467205586523\n",
            "Total Timesteps: 3824 Episode Num: 263 Reward: 13.802097324875652\n",
            "Total Timesteps: 3843 Episode Num: 264 Reward: 19.27522924697696\n",
            "Total Timesteps: 3858 Episode Num: 265 Reward: 17.770718998654047\n",
            "Total Timesteps: 3872 Episode Num: 266 Reward: 16.724658817604354\n",
            "Total Timesteps: 3893 Episode Num: 267 Reward: 16.699517596336957\n",
            "Total Timesteps: 3907 Episode Num: 268 Reward: 16.278578644566007\n",
            "Total Timesteps: 3930 Episode Num: 269 Reward: 23.019641951833915\n",
            "Total Timesteps: 3953 Episode Num: 270 Reward: 20.431561684889314\n",
            "Total Timesteps: 3961 Episode Num: 271 Reward: 13.517167375342979\n",
            "Total Timesteps: 3969 Episode Num: 272 Reward: 13.32969132819562\n",
            "Total Timesteps: 3982 Episode Num: 273 Reward: 18.761925320471345\n",
            "Total Timesteps: 4001 Episode Num: 274 Reward: 17.27764874118002\n",
            "Total Timesteps: 4022 Episode Num: 275 Reward: 18.593454394114087\n",
            "Total Timesteps: 4030 Episode Num: 276 Reward: 12.97929122091009\n",
            "Total Timesteps: 4044 Episode Num: 277 Reward: 13.355469515950242\n",
            "Total Timesteps: 4053 Episode Num: 278 Reward: 14.01426771885308\n",
            "Total Timesteps: 4061 Episode Num: 279 Reward: 12.601009402605996\n",
            "Total Timesteps: 4077 Episode Num: 280 Reward: 19.292925105680474\n",
            "Total Timesteps: 4089 Episode Num: 281 Reward: 15.582413206582714\n",
            "Total Timesteps: 4098 Episode Num: 282 Reward: 14.543670042438317\n",
            "Total Timesteps: 4112 Episode Num: 283 Reward: 18.416809563949936\n",
            "Total Timesteps: 4128 Episode Num: 284 Reward: 18.126028163169394\n",
            "Total Timesteps: 4138 Episode Num: 285 Reward: 13.735350086053948\n",
            "Total Timesteps: 4160 Episode Num: 286 Reward: 19.62950451982615\n",
            "Total Timesteps: 4171 Episode Num: 287 Reward: 16.09566883389052\n",
            "Total Timesteps: 4182 Episode Num: 288 Reward: 13.279659098420234\n",
            "Total Timesteps: 4219 Episode Num: 289 Reward: 35.797408534702846\n",
            "Total Timesteps: 4238 Episode Num: 290 Reward: 21.312126473877292\n",
            "Total Timesteps: 4259 Episode Num: 291 Reward: 19.49017289570475\n",
            "Total Timesteps: 4274 Episode Num: 292 Reward: 16.713143530608797\n",
            "Total Timesteps: 4293 Episode Num: 293 Reward: 18.77717114286934\n",
            "Total Timesteps: 4313 Episode Num: 294 Reward: 21.51526943452918\n",
            "Total Timesteps: 4329 Episode Num: 295 Reward: 19.225718932453308\n",
            "Total Timesteps: 4345 Episode Num: 296 Reward: 19.68472267800098\n",
            "Total Timesteps: 4356 Episode Num: 297 Reward: 14.305880562846141\n",
            "Total Timesteps: 4367 Episode Num: 298 Reward: 14.10738282668899\n",
            "Total Timesteps: 4379 Episode Num: 299 Reward: 16.58131858505076\n",
            "Total Timesteps: 4394 Episode Num: 300 Reward: 19.347966323024593\n",
            "Total Timesteps: 4407 Episode Num: 301 Reward: 15.286618380088475\n",
            "Total Timesteps: 4434 Episode Num: 302 Reward: 28.86447790254315\n",
            "Total Timesteps: 4448 Episode Num: 303 Reward: 15.427849973519917\n",
            "Total Timesteps: 4465 Episode Num: 304 Reward: 17.14153477989894\n",
            "Total Timesteps: 4479 Episode Num: 305 Reward: 19.234497043736337\n",
            "Total Timesteps: 4486 Episode Num: 306 Reward: 10.945996496255974\n",
            "Total Timesteps: 4506 Episode Num: 307 Reward: 23.13427387353877\n",
            "Total Timesteps: 4513 Episode Num: 308 Reward: 12.46743208675325\n",
            "Total Timesteps: 4526 Episode Num: 309 Reward: 16.612173702032308\n",
            "Total Timesteps: 4546 Episode Num: 310 Reward: 18.785951465182006\n",
            "Total Timesteps: 4553 Episode Num: 311 Reward: 13.289170764907611\n",
            "Total Timesteps: 4562 Episode Num: 312 Reward: 13.976920656193396\n",
            "Total Timesteps: 4572 Episode Num: 313 Reward: 15.411858895282787\n",
            "Total Timesteps: 4580 Episode Num: 314 Reward: 11.220734987031028\n",
            "Total Timesteps: 4591 Episode Num: 315 Reward: 16.11843988748733\n",
            "Total Timesteps: 4601 Episode Num: 316 Reward: 14.827104948615306\n",
            "Total Timesteps: 4613 Episode Num: 317 Reward: 15.488439384270169\n",
            "Total Timesteps: 4623 Episode Num: 318 Reward: 14.83094901440345\n",
            "Total Timesteps: 4634 Episode Num: 319 Reward: 15.432036825150135\n",
            "Total Timesteps: 4653 Episode Num: 320 Reward: 19.291184916684873\n",
            "Total Timesteps: 4663 Episode Num: 321 Reward: 15.62375907497335\n",
            "Total Timesteps: 4690 Episode Num: 322 Reward: 26.711005311952615\n",
            "Total Timesteps: 4705 Episode Num: 323 Reward: 17.500215665514407\n",
            "Total Timesteps: 4718 Episode Num: 324 Reward: 16.394846902330755\n",
            "Total Timesteps: 4726 Episode Num: 325 Reward: 12.140331061041799\n",
            "Total Timesteps: 4742 Episode Num: 326 Reward: 17.836820461561732\n",
            "Total Timesteps: 4752 Episode Num: 327 Reward: 14.468323106339081\n",
            "Total Timesteps: 4766 Episode Num: 328 Reward: 17.846792424476007\n",
            "Total Timesteps: 4784 Episode Num: 329 Reward: 20.34757303653169\n",
            "Total Timesteps: 4791 Episode Num: 330 Reward: 12.729890135218739\n",
            "Total Timesteps: 4809 Episode Num: 331 Reward: 16.366423041491366\n",
            "Total Timesteps: 4823 Episode Num: 332 Reward: 16.808249421218353\n",
            "Total Timesteps: 4830 Episode Num: 333 Reward: 12.905066171867658\n",
            "Total Timesteps: 4847 Episode Num: 334 Reward: 20.45365988076519\n",
            "Total Timesteps: 4858 Episode Num: 335 Reward: 15.840858020597079\n",
            "Total Timesteps: 4872 Episode Num: 336 Reward: 17.20169109720737\n",
            "Total Timesteps: 4887 Episode Num: 337 Reward: 17.500185636368403\n",
            "Total Timesteps: 4906 Episode Num: 338 Reward: 20.87791741103283\n",
            "Total Timesteps: 4916 Episode Num: 339 Reward: 14.089044401164573\n",
            "Total Timesteps: 4936 Episode Num: 340 Reward: 22.869207441453177\n",
            "Total Timesteps: 4953 Episode Num: 341 Reward: 17.921754692673858\n",
            "Total Timesteps: 4968 Episode Num: 342 Reward: 18.678200684882174\n",
            "Total Timesteps: 4991 Episode Num: 343 Reward: 25.427283066902604\n",
            "Total Timesteps: 5003 Episode Num: 344 Reward: 14.99526220618136\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 6.162212\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 6.179226\n",
            "---------------------------------------\n",
            "Total Timesteps: 5014 Episode Num: 345 Reward: 15.599182242424286\n",
            "Total Timesteps: 5029 Episode Num: 346 Reward: 17.847833645444187\n",
            "Total Timesteps: 5039 Episode Num: 347 Reward: 15.115993119617631\n",
            "Total Timesteps: 5067 Episode Num: 348 Reward: 24.774259162730598\n",
            "Total Timesteps: 5080 Episode Num: 349 Reward: 14.197412800817984\n",
            "Total Timesteps: 5089 Episode Num: 350 Reward: 13.76383058854117\n",
            "Total Timesteps: 5100 Episode Num: 351 Reward: 15.212498786095239\n",
            "Total Timesteps: 5116 Episode Num: 352 Reward: 17.951842319763088\n",
            "Total Timesteps: 5132 Episode Num: 353 Reward: 15.576669272857545\n",
            "Total Timesteps: 5143 Episode Num: 354 Reward: 14.172076568898047\n",
            "Total Timesteps: 5154 Episode Num: 355 Reward: 15.998293998491135\n",
            "Total Timesteps: 5164 Episode Num: 356 Reward: 13.538031959437651\n",
            "Total Timesteps: 5175 Episode Num: 357 Reward: 14.827946303362841\n",
            "Total Timesteps: 5203 Episode Num: 358 Reward: 21.451543000376837\n",
            "Total Timesteps: 5224 Episode Num: 359 Reward: 22.377202194680283\n",
            "Total Timesteps: 5237 Episode Num: 360 Reward: 15.617203838622661\n",
            "Total Timesteps: 5246 Episode Num: 361 Reward: 14.36646890011907\n",
            "Total Timesteps: 5260 Episode Num: 362 Reward: 14.65679368768033\n",
            "Total Timesteps: 5271 Episode Num: 363 Reward: 14.503960496360378\n",
            "Total Timesteps: 5290 Episode Num: 364 Reward: 18.378882427702774\n",
            "Total Timesteps: 5298 Episode Num: 365 Reward: 13.477377047289336\n",
            "Total Timesteps: 5341 Episode Num: 366 Reward: 33.86425030675309\n",
            "Total Timesteps: 5354 Episode Num: 367 Reward: 13.528924020139675\n",
            "Total Timesteps: 5364 Episode Num: 368 Reward: 13.764236314213488\n",
            "Total Timesteps: 5380 Episode Num: 369 Reward: 19.53116299319954\n",
            "Total Timesteps: 5395 Episode Num: 370 Reward: 14.617314501435612\n",
            "Total Timesteps: 5409 Episode Num: 371 Reward: 17.77353055080021\n",
            "Total Timesteps: 5434 Episode Num: 372 Reward: 21.77858582994377\n",
            "Total Timesteps: 5464 Episode Num: 373 Reward: 25.694236834360346\n",
            "Total Timesteps: 5488 Episode Num: 374 Reward: 19.910832389559065\n",
            "Total Timesteps: 5503 Episode Num: 375 Reward: 17.43347803795041\n",
            "Total Timesteps: 5511 Episode Num: 376 Reward: 12.68150857057044\n",
            "Total Timesteps: 5521 Episode Num: 377 Reward: 14.234022248949621\n",
            "Total Timesteps: 5543 Episode Num: 378 Reward: 25.080982671825044\n",
            "Total Timesteps: 5554 Episode Num: 379 Reward: 15.028684386679378\n",
            "Total Timesteps: 5565 Episode Num: 380 Reward: 15.732674665009833\n",
            "Total Timesteps: 5578 Episode Num: 381 Reward: 17.510159551681134\n",
            "Total Timesteps: 5587 Episode Num: 382 Reward: 13.455114373553078\n",
            "Total Timesteps: 5597 Episode Num: 383 Reward: 13.17820742009062\n",
            "Total Timesteps: 5612 Episode Num: 384 Reward: 15.711078528972575\n",
            "Total Timesteps: 5630 Episode Num: 385 Reward: 19.42507008418179\n",
            "Total Timesteps: 5647 Episode Num: 386 Reward: 18.82652386299014\n",
            "Total Timesteps: 5662 Episode Num: 387 Reward: 16.087391613348153\n",
            "Total Timesteps: 5681 Episode Num: 388 Reward: 22.558084883574338\n",
            "Total Timesteps: 5697 Episode Num: 389 Reward: 15.688828259412546\n",
            "Total Timesteps: 5714 Episode Num: 390 Reward: 19.79604698264011\n",
            "Total Timesteps: 5732 Episode Num: 391 Reward: 19.850454495962186\n",
            "Total Timesteps: 5745 Episode Num: 392 Reward: 14.750704423035494\n",
            "Total Timesteps: 5762 Episode Num: 393 Reward: 17.320528661865687\n",
            "Total Timesteps: 5774 Episode Num: 394 Reward: 14.974885665914915\n",
            "Total Timesteps: 5792 Episode Num: 395 Reward: 19.701107023947408\n",
            "Total Timesteps: 5800 Episode Num: 396 Reward: 12.68720468213578\n",
            "Total Timesteps: 5815 Episode Num: 397 Reward: 17.258658331319747\n",
            "Total Timesteps: 5822 Episode Num: 398 Reward: 11.21447641543782\n",
            "Total Timesteps: 5834 Episode Num: 399 Reward: 16.174462513248727\n",
            "Total Timesteps: 5849 Episode Num: 400 Reward: 17.275487889126815\n",
            "Total Timesteps: 5865 Episode Num: 401 Reward: 18.154364781024928\n",
            "Total Timesteps: 5877 Episode Num: 402 Reward: 14.651729978260118\n",
            "Total Timesteps: 5884 Episode Num: 403 Reward: 10.472630915527407\n",
            "Total Timesteps: 5900 Episode Num: 404 Reward: 16.870831626809377\n",
            "Total Timesteps: 5920 Episode Num: 405 Reward: 21.79261638041789\n",
            "Total Timesteps: 5933 Episode Num: 406 Reward: 14.719697960527267\n",
            "Total Timesteps: 5944 Episode Num: 407 Reward: 14.1993530943073\n",
            "Total Timesteps: 5957 Episode Num: 408 Reward: 16.162696336097724\n",
            "Total Timesteps: 5971 Episode Num: 409 Reward: 16.202873391260802\n",
            "Total Timesteps: 5979 Episode Num: 410 Reward: 12.651042760493873\n",
            "Total Timesteps: 5989 Episode Num: 411 Reward: 15.553125187233674\n",
            "Total Timesteps: 5997 Episode Num: 412 Reward: 14.02369329810026\n",
            "Total Timesteps: 6013 Episode Num: 413 Reward: 17.0373269278527\n",
            "Total Timesteps: 6030 Episode Num: 414 Reward: 19.251542367953512\n",
            "Total Timesteps: 6042 Episode Num: 415 Reward: 16.24905875235272\n",
            "Total Timesteps: 6062 Episode Num: 416 Reward: 17.371920148104252\n",
            "Total Timesteps: 6081 Episode Num: 417 Reward: 23.752595206887058\n",
            "Total Timesteps: 6089 Episode Num: 418 Reward: 12.324671915196813\n",
            "Total Timesteps: 6102 Episode Num: 419 Reward: 15.298789871788175\n",
            "Total Timesteps: 6110 Episode Num: 420 Reward: 13.363176603952889\n",
            "Total Timesteps: 6122 Episode Num: 421 Reward: 17.82618761559279\n",
            "Total Timesteps: 6135 Episode Num: 422 Reward: 16.98934039237502\n",
            "Total Timesteps: 6144 Episode Num: 423 Reward: 12.25772558273602\n",
            "Total Timesteps: 6161 Episode Num: 424 Reward: 18.53260340490378\n",
            "Total Timesteps: 6180 Episode Num: 425 Reward: 17.962633629258193\n",
            "Total Timesteps: 6199 Episode Num: 426 Reward: 18.882419433532053\n",
            "Total Timesteps: 6221 Episode Num: 427 Reward: 20.69117371050816\n",
            "Total Timesteps: 6232 Episode Num: 428 Reward: 13.453937854728427\n",
            "Total Timesteps: 6241 Episode Num: 429 Reward: 15.795691040517703\n",
            "Total Timesteps: 6257 Episode Num: 430 Reward: 16.41861398486799\n",
            "Total Timesteps: 6273 Episode Num: 431 Reward: 19.0949205656565\n",
            "Total Timesteps: 6288 Episode Num: 432 Reward: 14.775590397193447\n",
            "Total Timesteps: 6300 Episode Num: 433 Reward: 18.059013253777813\n",
            "Total Timesteps: 6320 Episode Num: 434 Reward: 19.63387300869654\n",
            "Total Timesteps: 6331 Episode Num: 435 Reward: 14.85106715578731\n",
            "Total Timesteps: 6349 Episode Num: 436 Reward: 19.890302233200053\n",
            "Total Timesteps: 6360 Episode Num: 437 Reward: 15.648086239089025\n",
            "Total Timesteps: 6372 Episode Num: 438 Reward: 15.712149659059651\n",
            "Total Timesteps: 6390 Episode Num: 439 Reward: 21.110755032891756\n",
            "Total Timesteps: 6399 Episode Num: 440 Reward: 12.984716030418348\n",
            "Total Timesteps: 6426 Episode Num: 441 Reward: 26.501029455810198\n",
            "Total Timesteps: 6451 Episode Num: 442 Reward: 22.88663265532231\n",
            "Total Timesteps: 6476 Episode Num: 443 Reward: 25.915171581966575\n",
            "Total Timesteps: 6487 Episode Num: 444 Reward: 13.15090265314648\n",
            "Total Timesteps: 6496 Episode Num: 445 Reward: 14.355172878048323\n",
            "Total Timesteps: 6508 Episode Num: 446 Reward: 15.982358140668657\n",
            "Total Timesteps: 6519 Episode Num: 447 Reward: 15.740175281434494\n",
            "Total Timesteps: 6536 Episode Num: 448 Reward: 17.629791712514997\n",
            "Total Timesteps: 6553 Episode Num: 449 Reward: 17.73102948362794\n",
            "Total Timesteps: 6578 Episode Num: 450 Reward: 22.129924458633464\n",
            "Total Timesteps: 6589 Episode Num: 451 Reward: 15.186318903937352\n",
            "Total Timesteps: 6599 Episode Num: 452 Reward: 14.275184015443664\n",
            "Total Timesteps: 6616 Episode Num: 453 Reward: 18.01822427304578\n",
            "Total Timesteps: 6640 Episode Num: 454 Reward: 18.34809365369146\n",
            "Total Timesteps: 6652 Episode Num: 455 Reward: 14.079371585855549\n",
            "Total Timesteps: 6676 Episode Num: 456 Reward: 18.397101487001056\n",
            "Total Timesteps: 6692 Episode Num: 457 Reward: 18.406872649290015\n",
            "Total Timesteps: 6713 Episode Num: 458 Reward: 23.734834397184024\n",
            "Total Timesteps: 6726 Episode Num: 459 Reward: 16.584185528595118\n",
            "Total Timesteps: 6739 Episode Num: 460 Reward: 17.321612303417353\n",
            "Total Timesteps: 6750 Episode Num: 461 Reward: 14.852276068271022\n",
            "Total Timesteps: 6759 Episode Num: 462 Reward: 13.993817235417374\n",
            "Total Timesteps: 6769 Episode Num: 463 Reward: 15.137519533172599\n",
            "Total Timesteps: 6780 Episode Num: 464 Reward: 14.913239723027802\n",
            "Total Timesteps: 6804 Episode Num: 465 Reward: 22.99009818674531\n",
            "Total Timesteps: 6817 Episode Num: 466 Reward: 15.110662689637683\n",
            "Total Timesteps: 6837 Episode Num: 467 Reward: 18.02168486664886\n",
            "Total Timesteps: 6854 Episode Num: 468 Reward: 16.988209054706385\n",
            "Total Timesteps: 6868 Episode Num: 469 Reward: 17.16782403194811\n",
            "Total Timesteps: 6882 Episode Num: 470 Reward: 16.869804300331452\n",
            "Total Timesteps: 6890 Episode Num: 471 Reward: 12.313423812984547\n",
            "Total Timesteps: 6900 Episode Num: 472 Reward: 15.094877700124925\n",
            "Total Timesteps: 6916 Episode Num: 473 Reward: 15.225203001484623\n",
            "Total Timesteps: 6928 Episode Num: 474 Reward: 15.822854404285316\n",
            "Total Timesteps: 6940 Episode Num: 475 Reward: 15.487662900125727\n",
            "Total Timesteps: 6952 Episode Num: 476 Reward: 15.317635649512521\n",
            "Total Timesteps: 6968 Episode Num: 477 Reward: 18.697772381358664\n",
            "Total Timesteps: 6976 Episode Num: 478 Reward: 13.653478412039114\n",
            "Total Timesteps: 6985 Episode Num: 479 Reward: 11.99195663712453\n",
            "Total Timesteps: 6996 Episode Num: 480 Reward: 15.970841695264975\n",
            "Total Timesteps: 7011 Episode Num: 481 Reward: 16.446146489282544\n",
            "Total Timesteps: 7022 Episode Num: 482 Reward: 15.77771337472368\n",
            "Total Timesteps: 7037 Episode Num: 483 Reward: 16.348278460670556\n",
            "Total Timesteps: 7051 Episode Num: 484 Reward: 17.077449150325265\n",
            "Total Timesteps: 7061 Episode Num: 485 Reward: 13.750407277287742\n",
            "Total Timesteps: 7069 Episode Num: 486 Reward: 12.155168399744435\n",
            "Total Timesteps: 7085 Episode Num: 487 Reward: 15.437437778706952\n",
            "Total Timesteps: 7094 Episode Num: 488 Reward: 12.681392057886114\n",
            "Total Timesteps: 7102 Episode Num: 489 Reward: 11.253924784822447\n",
            "Total Timesteps: 7117 Episode Num: 490 Reward: 17.195289421499183\n",
            "Total Timesteps: 7128 Episode Num: 491 Reward: 14.667515680049837\n",
            "Total Timesteps: 7140 Episode Num: 492 Reward: 15.501885003024652\n",
            "Total Timesteps: 7148 Episode Num: 493 Reward: 12.487721796809636\n",
            "Total Timesteps: 7166 Episode Num: 494 Reward: 16.080463141093787\n",
            "Total Timesteps: 7177 Episode Num: 495 Reward: 14.658806439759791\n",
            "Total Timesteps: 7187 Episode Num: 496 Reward: 13.162468563840957\n",
            "Total Timesteps: 7196 Episode Num: 497 Reward: 15.304959499429968\n",
            "Total Timesteps: 7221 Episode Num: 498 Reward: 25.66074878930958\n",
            "Total Timesteps: 7233 Episode Num: 499 Reward: 15.788478786179619\n",
            "Total Timesteps: 7251 Episode Num: 500 Reward: 16.85234947876597\n",
            "Total Timesteps: 7259 Episode Num: 501 Reward: 12.762849725299747\n",
            "Total Timesteps: 7269 Episode Num: 502 Reward: 15.332367018704826\n",
            "Total Timesteps: 7282 Episode Num: 503 Reward: 17.020304728878546\n",
            "Total Timesteps: 7300 Episode Num: 504 Reward: 18.57277162171231\n",
            "Total Timesteps: 7309 Episode Num: 505 Reward: 13.033698423938766\n",
            "Total Timesteps: 7322 Episode Num: 506 Reward: 16.658596287546967\n",
            "Total Timesteps: 7332 Episode Num: 507 Reward: 15.258282055251765\n",
            "Total Timesteps: 7343 Episode Num: 508 Reward: 15.484176418652348\n",
            "Total Timesteps: 7361 Episode Num: 509 Reward: 20.253288783965395\n",
            "Total Timesteps: 7369 Episode Num: 510 Reward: 13.276752560460592\n",
            "Total Timesteps: 7387 Episode Num: 511 Reward: 19.491524071288588\n",
            "Total Timesteps: 7406 Episode Num: 512 Reward: 20.82791848957568\n",
            "Total Timesteps: 7428 Episode Num: 513 Reward: 21.067762291293185\n",
            "Total Timesteps: 7438 Episode Num: 514 Reward: 14.49669106364017\n",
            "Total Timesteps: 7449 Episode Num: 515 Reward: 16.649814282753507\n",
            "Total Timesteps: 7468 Episode Num: 516 Reward: 19.350323901047524\n",
            "Total Timesteps: 7478 Episode Num: 517 Reward: 15.195619966776576\n",
            "Total Timesteps: 7486 Episode Num: 518 Reward: 13.539115978870539\n",
            "Total Timesteps: 7496 Episode Num: 519 Reward: 13.053548901931208\n",
            "Total Timesteps: 7509 Episode Num: 520 Reward: 15.924218004582505\n",
            "Total Timesteps: 7529 Episode Num: 521 Reward: 23.068260406100308\n",
            "Total Timesteps: 7547 Episode Num: 522 Reward: 19.315976432028403\n",
            "Total Timesteps: 7558 Episode Num: 523 Reward: 16.584093316376673\n",
            "Total Timesteps: 7579 Episode Num: 524 Reward: 22.50683961202449\n",
            "Total Timesteps: 7600 Episode Num: 525 Reward: 22.941179349273444\n",
            "Total Timesteps: 7623 Episode Num: 526 Reward: 21.894924808832005\n",
            "Total Timesteps: 7633 Episode Num: 527 Reward: 14.579742433558568\n",
            "Total Timesteps: 7641 Episode Num: 528 Reward: 12.787708057682902\n",
            "Total Timesteps: 7651 Episode Num: 529 Reward: 14.152017727580095\n",
            "Total Timesteps: 7661 Episode Num: 530 Reward: 14.809743201472156\n",
            "Total Timesteps: 7674 Episode Num: 531 Reward: 14.042834995494921\n",
            "Total Timesteps: 7682 Episode Num: 532 Reward: 14.033450585122045\n",
            "Total Timesteps: 7694 Episode Num: 533 Reward: 14.78907772734383\n",
            "Total Timesteps: 7702 Episode Num: 534 Reward: 12.633892015158198\n",
            "Total Timesteps: 7720 Episode Num: 535 Reward: 17.37348602636339\n",
            "Total Timesteps: 7730 Episode Num: 536 Reward: 14.042718398864961\n",
            "Total Timesteps: 7741 Episode Num: 537 Reward: 14.77756144905143\n",
            "Total Timesteps: 7752 Episode Num: 538 Reward: 14.595298765112236\n",
            "Total Timesteps: 7766 Episode Num: 539 Reward: 16.753402198100233\n",
            "Total Timesteps: 7775 Episode Num: 540 Reward: 13.644165396640892\n",
            "Total Timesteps: 7784 Episode Num: 541 Reward: 14.02615892874019\n",
            "Total Timesteps: 7798 Episode Num: 542 Reward: 18.454529695028032\n",
            "Total Timesteps: 7807 Episode Num: 543 Reward: 13.939321516171914\n",
            "Total Timesteps: 7821 Episode Num: 544 Reward: 16.964033017522887\n",
            "Total Timesteps: 7840 Episode Num: 545 Reward: 18.772617847708165\n",
            "Total Timesteps: 7855 Episode Num: 546 Reward: 17.893971523513027\n",
            "Total Timesteps: 7873 Episode Num: 547 Reward: 16.478563801155538\n",
            "Total Timesteps: 7885 Episode Num: 548 Reward: 16.1491405670924\n",
            "Total Timesteps: 7896 Episode Num: 549 Reward: 16.021638491065822\n",
            "Total Timesteps: 7910 Episode Num: 550 Reward: 17.0730892048261\n",
            "Total Timesteps: 7925 Episode Num: 551 Reward: 17.22530122941389\n",
            "Total Timesteps: 7936 Episode Num: 552 Reward: 13.834678029300996\n",
            "Total Timesteps: 7961 Episode Num: 553 Reward: 22.532657431163532\n",
            "Total Timesteps: 7980 Episode Num: 554 Reward: 17.0748970791843\n",
            "Total Timesteps: 7990 Episode Num: 555 Reward: 14.227468200218574\n",
            "Total Timesteps: 8003 Episode Num: 556 Reward: 15.5095285315474\n",
            "Total Timesteps: 8014 Episode Num: 557 Reward: 13.423712224209158\n",
            "Total Timesteps: 8028 Episode Num: 558 Reward: 17.13533652564365\n",
            "Total Timesteps: 8038 Episode Num: 559 Reward: 14.020410638586325\n",
            "Total Timesteps: 8047 Episode Num: 560 Reward: 12.940756867369055\n",
            "Total Timesteps: 8064 Episode Num: 561 Reward: 18.056926724901132\n",
            "Total Timesteps: 8073 Episode Num: 562 Reward: 12.641381618946616\n",
            "Total Timesteps: 8083 Episode Num: 563 Reward: 12.790786751118139\n",
            "Total Timesteps: 8101 Episode Num: 564 Reward: 20.73900172821304\n",
            "Total Timesteps: 8114 Episode Num: 565 Reward: 17.34161303250148\n",
            "Total Timesteps: 8125 Episode Num: 566 Reward: 15.264713440030755\n",
            "Total Timesteps: 8134 Episode Num: 567 Reward: 13.1620944691138\n",
            "Total Timesteps: 8143 Episode Num: 568 Reward: 13.714992618825635\n",
            "Total Timesteps: 8154 Episode Num: 569 Reward: 14.75529635135026\n",
            "Total Timesteps: 8166 Episode Num: 570 Reward: 16.416952709539327\n",
            "Total Timesteps: 8182 Episode Num: 571 Reward: 17.38766474892036\n",
            "Total Timesteps: 8190 Episode Num: 572 Reward: 13.359603520088422\n",
            "Total Timesteps: 8200 Episode Num: 573 Reward: 15.760639325996454\n",
            "Total Timesteps: 8219 Episode Num: 574 Reward: 20.5677199480444\n",
            "Total Timesteps: 8231 Episode Num: 575 Reward: 15.009364709358486\n",
            "Total Timesteps: 8240 Episode Num: 576 Reward: 12.552357036620378\n",
            "Total Timesteps: 8250 Episode Num: 577 Reward: 14.742191869969247\n",
            "Total Timesteps: 8268 Episode Num: 578 Reward: 21.598503434007576\n",
            "Total Timesteps: 8280 Episode Num: 579 Reward: 14.917041291126226\n",
            "Total Timesteps: 8297 Episode Num: 580 Reward: 19.307111540499317\n",
            "Total Timesteps: 8320 Episode Num: 581 Reward: 22.89269581756816\n",
            "Total Timesteps: 8330 Episode Num: 582 Reward: 13.00014703660563\n",
            "Total Timesteps: 8338 Episode Num: 583 Reward: 13.207281913135375\n",
            "Total Timesteps: 8354 Episode Num: 584 Reward: 16.121410691255004\n",
            "Total Timesteps: 8372 Episode Num: 585 Reward: 15.375297093401603\n",
            "Total Timesteps: 8380 Episode Num: 586 Reward: 12.595871144533158\n",
            "Total Timesteps: 8399 Episode Num: 587 Reward: 21.592696878140853\n",
            "Total Timesteps: 8414 Episode Num: 588 Reward: 16.90256238238071\n",
            "Total Timesteps: 8425 Episode Num: 589 Reward: 16.05417666978028\n",
            "Total Timesteps: 8435 Episode Num: 590 Reward: 12.491309067803376\n",
            "Total Timesteps: 8446 Episode Num: 591 Reward: 14.810931450365754\n",
            "Total Timesteps: 8464 Episode Num: 592 Reward: 21.75391650105448\n",
            "Total Timesteps: 8491 Episode Num: 593 Reward: 25.840822216651585\n",
            "Total Timesteps: 8500 Episode Num: 594 Reward: 14.40150992382114\n",
            "Total Timesteps: 8522 Episode Num: 595 Reward: 24.065399948364938\n",
            "Total Timesteps: 8534 Episode Num: 596 Reward: 14.568420095471083\n",
            "Total Timesteps: 8542 Episode Num: 597 Reward: 12.313231721481133\n",
            "Total Timesteps: 8556 Episode Num: 598 Reward: 16.286405628368083\n",
            "Total Timesteps: 8566 Episode Num: 599 Reward: 12.734188195707974\n",
            "Total Timesteps: 8581 Episode Num: 600 Reward: 19.33186324555136\n",
            "Total Timesteps: 8597 Episode Num: 601 Reward: 17.949084108731768\n",
            "Total Timesteps: 8609 Episode Num: 602 Reward: 16.801388661369856\n",
            "Total Timesteps: 8620 Episode Num: 603 Reward: 15.417665746739658\n",
            "Total Timesteps: 8643 Episode Num: 604 Reward: 24.217355445338757\n",
            "Total Timesteps: 8652 Episode Num: 605 Reward: 12.152371395957015\n",
            "Total Timesteps: 8671 Episode Num: 606 Reward: 18.26539949807775\n",
            "Total Timesteps: 8681 Episode Num: 607 Reward: 14.132246350464992\n",
            "Total Timesteps: 8688 Episode Num: 608 Reward: 11.35480948651966\n",
            "Total Timesteps: 8700 Episode Num: 609 Reward: 16.530018034573004\n",
            "Total Timesteps: 8721 Episode Num: 610 Reward: 23.46256822576979\n",
            "Total Timesteps: 8742 Episode Num: 611 Reward: 19.142722286166098\n",
            "Total Timesteps: 8759 Episode Num: 612 Reward: 13.172635903199259\n",
            "Total Timesteps: 8767 Episode Num: 613 Reward: 12.069514072290621\n",
            "Total Timesteps: 8778 Episode Num: 614 Reward: 13.526592721862835\n",
            "Total Timesteps: 8791 Episode Num: 615 Reward: 17.105861513124545\n",
            "Total Timesteps: 8806 Episode Num: 616 Reward: 18.402086662974035\n",
            "Total Timesteps: 8818 Episode Num: 617 Reward: 16.895333718259643\n",
            "Total Timesteps: 8828 Episode Num: 618 Reward: 14.608922592680026\n",
            "Total Timesteps: 8843 Episode Num: 619 Reward: 14.237919658706234\n",
            "Total Timesteps: 8853 Episode Num: 620 Reward: 13.39416484671674\n",
            "Total Timesteps: 8864 Episode Num: 621 Reward: 15.303049342105805\n",
            "Total Timesteps: 8873 Episode Num: 622 Reward: 13.930219434686295\n",
            "Total Timesteps: 8882 Episode Num: 623 Reward: 13.696500380507494\n",
            "Total Timesteps: 8898 Episode Num: 624 Reward: 16.38639985588961\n",
            "Total Timesteps: 8919 Episode Num: 625 Reward: 18.87212488982187\n",
            "Total Timesteps: 8930 Episode Num: 626 Reward: 14.984737418158328\n",
            "Total Timesteps: 8943 Episode Num: 627 Reward: 15.087700930410938\n",
            "Total Timesteps: 8958 Episode Num: 628 Reward: 18.91622967648\n",
            "Total Timesteps: 8975 Episode Num: 629 Reward: 17.182483447931006\n",
            "Total Timesteps: 8985 Episode Num: 630 Reward: 14.881433017959353\n",
            "Total Timesteps: 9003 Episode Num: 631 Reward: 20.5302647814955\n",
            "Total Timesteps: 9011 Episode Num: 632 Reward: 12.765361804942948\n",
            "Total Timesteps: 9021 Episode Num: 633 Reward: 13.951541684765834\n",
            "Total Timesteps: 9028 Episode Num: 634 Reward: 11.752581566710433\n",
            "Total Timesteps: 9038 Episode Num: 635 Reward: 14.64292750308523\n",
            "Total Timesteps: 9049 Episode Num: 636 Reward: 15.327480844109958\n",
            "Total Timesteps: 9074 Episode Num: 637 Reward: 22.591049329067754\n",
            "Total Timesteps: 9089 Episode Num: 638 Reward: 15.604239229652737\n",
            "Total Timesteps: 9099 Episode Num: 639 Reward: 13.815296721419143\n",
            "Total Timesteps: 9110 Episode Num: 640 Reward: 15.342718327246256\n",
            "Total Timesteps: 9126 Episode Num: 641 Reward: 16.793359705734474\n",
            "Total Timesteps: 9142 Episode Num: 642 Reward: 16.35600847866881\n",
            "Total Timesteps: 9158 Episode Num: 643 Reward: 19.362172946351347\n",
            "Total Timesteps: 9167 Episode Num: 644 Reward: 11.129734937708418\n",
            "Total Timesteps: 9183 Episode Num: 645 Reward: 17.550344030740963\n",
            "Total Timesteps: 9194 Episode Num: 646 Reward: 14.327229352886206\n",
            "Total Timesteps: 9205 Episode Num: 647 Reward: 15.02192065931449\n",
            "Total Timesteps: 9216 Episode Num: 648 Reward: 12.883893015592186\n",
            "Total Timesteps: 9229 Episode Num: 649 Reward: 16.301909162531953\n",
            "Total Timesteps: 9240 Episode Num: 650 Reward: 14.676985389149923\n",
            "Total Timesteps: 9255 Episode Num: 651 Reward: 17.402506122401974\n",
            "Total Timesteps: 9267 Episode Num: 652 Reward: 15.721437549019177\n",
            "Total Timesteps: 9277 Episode Num: 653 Reward: 15.259541881190673\n",
            "Total Timesteps: 9301 Episode Num: 654 Reward: 20.219248403921668\n",
            "Total Timesteps: 9315 Episode Num: 655 Reward: 19.086423858397755\n",
            "Total Timesteps: 9338 Episode Num: 656 Reward: 21.60926197869994\n",
            "Total Timesteps: 9351 Episode Num: 657 Reward: 14.255572797400236\n",
            "Total Timesteps: 9373 Episode Num: 658 Reward: 19.68753173509176\n",
            "Total Timesteps: 9390 Episode Num: 659 Reward: 20.078139182798623\n",
            "Total Timesteps: 9402 Episode Num: 660 Reward: 14.343853886770376\n",
            "Total Timesteps: 9411 Episode Num: 661 Reward: 12.34503589846281\n",
            "Total Timesteps: 9424 Episode Num: 662 Reward: 18.11620366110728\n",
            "Total Timesteps: 9437 Episode Num: 663 Reward: 15.752589759827243\n",
            "Total Timesteps: 9449 Episode Num: 664 Reward: 14.767432269315758\n",
            "Total Timesteps: 9468 Episode Num: 665 Reward: 17.652255376304673\n",
            "Total Timesteps: 9478 Episode Num: 666 Reward: 14.943167865695433\n",
            "Total Timesteps: 9492 Episode Num: 667 Reward: 16.151460375395256\n",
            "Total Timesteps: 9502 Episode Num: 668 Reward: 13.931999276186977\n",
            "Total Timesteps: 9529 Episode Num: 669 Reward: 26.845456610069967\n",
            "Total Timesteps: 9537 Episode Num: 670 Reward: 12.179393445480674\n",
            "Total Timesteps: 9555 Episode Num: 671 Reward: 19.636506434186593\n",
            "Total Timesteps: 9567 Episode Num: 672 Reward: 13.470219165548041\n",
            "Total Timesteps: 9580 Episode Num: 673 Reward: 15.491372441918067\n",
            "Total Timesteps: 9595 Episode Num: 674 Reward: 18.62544881722424\n",
            "Total Timesteps: 9603 Episode Num: 675 Reward: 11.924693945155013\n",
            "Total Timesteps: 9614 Episode Num: 676 Reward: 14.94757941623684\n",
            "Total Timesteps: 9629 Episode Num: 677 Reward: 18.283256554888794\n",
            "Total Timesteps: 9643 Episode Num: 678 Reward: 17.071161774307253\n",
            "Total Timesteps: 9652 Episode Num: 679 Reward: 13.705326763865015\n",
            "Total Timesteps: 9675 Episode Num: 680 Reward: 25.972762536334635\n",
            "Total Timesteps: 9697 Episode Num: 681 Reward: 21.09402632235433\n",
            "Total Timesteps: 9708 Episode Num: 682 Reward: 14.239747348212404\n",
            "Total Timesteps: 9728 Episode Num: 683 Reward: 20.16419079647458\n",
            "Total Timesteps: 9736 Episode Num: 684 Reward: 13.213614898588276\n",
            "Total Timesteps: 9758 Episode Num: 685 Reward: 20.345899424295933\n",
            "Total Timesteps: 9764 Episode Num: 686 Reward: 11.355499568016967\n",
            "Total Timesteps: 9778 Episode Num: 687 Reward: 16.00904165373795\n",
            "Total Timesteps: 9799 Episode Num: 688 Reward: 18.09560168905591\n",
            "Total Timesteps: 9832 Episode Num: 689 Reward: 24.005902095195783\n",
            "Total Timesteps: 9849 Episode Num: 690 Reward: 17.618088481233286\n",
            "Total Timesteps: 9860 Episode Num: 691 Reward: 14.759342975793695\n",
            "Total Timesteps: 9871 Episode Num: 692 Reward: 13.225697524614224\n",
            "Total Timesteps: 9881 Episode Num: 693 Reward: 15.698300444398773\n",
            "Total Timesteps: 9897 Episode Num: 694 Reward: 17.415592076642497\n",
            "Total Timesteps: 9920 Episode Num: 695 Reward: 24.289013640015032\n",
            "Total Timesteps: 9941 Episode Num: 696 Reward: 19.4597134600408\n",
            "Total Timesteps: 9954 Episode Num: 697 Reward: 14.478729834641854\n",
            "Total Timesteps: 9968 Episode Num: 698 Reward: 17.607331924648314\n",
            "Total Timesteps: 9981 Episode Num: 699 Reward: 17.62347575751046\n",
            "Total Timesteps: 9990 Episode Num: 700 Reward: 12.770407444666489\n",
            "Total Timesteps: 10004 Episode Num: 701 Reward: 13.960943974517926\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 6.137972\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 6.271622\n",
            "---------------------------------------\n",
            "Total Timesteps: 10013 Episode Num: 702 Reward: 7.06378531176403\n",
            "Total Timesteps: 10022 Episode Num: 703 Reward: 7.489357419445771\n",
            "Total Timesteps: 10031 Episode Num: 704 Reward: 6.55708454868082\n",
            "Total Timesteps: 10040 Episode Num: 705 Reward: 7.113859312192667\n",
            "Total Timesteps: 10049 Episode Num: 706 Reward: 6.001240806655801\n",
            "Total Timesteps: 10058 Episode Num: 707 Reward: 7.621496957851509\n",
            "Total Timesteps: 10067 Episode Num: 708 Reward: 6.685568822928585\n",
            "Total Timesteps: 10076 Episode Num: 709 Reward: 7.279809545388053\n",
            "Total Timesteps: 10085 Episode Num: 710 Reward: 6.3138384904487515\n",
            "Total Timesteps: 10095 Episode Num: 711 Reward: 6.590646471212281\n",
            "Total Timesteps: 10104 Episode Num: 712 Reward: 7.169597924005325\n",
            "Total Timesteps: 10113 Episode Num: 713 Reward: 6.481849901930659\n",
            "Total Timesteps: 10123 Episode Num: 714 Reward: 7.628927709767267\n",
            "Total Timesteps: 10132 Episode Num: 715 Reward: 6.593283049483651\n",
            "Total Timesteps: 10142 Episode Num: 716 Reward: 6.47534955690269\n",
            "Total Timesteps: 10151 Episode Num: 717 Reward: 7.065604452275909\n",
            "Total Timesteps: 10161 Episode Num: 718 Reward: 7.629556857570054\n",
            "Total Timesteps: 10171 Episode Num: 719 Reward: 7.653940291224021\n",
            "Total Timesteps: 10181 Episode Num: 720 Reward: 6.3483372803438645\n",
            "Total Timesteps: 10190 Episode Num: 721 Reward: 6.732052475266566\n",
            "Total Timesteps: 10200 Episode Num: 722 Reward: 7.615406954882959\n",
            "Total Timesteps: 10210 Episode Num: 723 Reward: 7.0136811126088165\n",
            "Total Timesteps: 10219 Episode Num: 724 Reward: 6.406752917481255\n",
            "Total Timesteps: 10228 Episode Num: 725 Reward: 6.734348414679009\n",
            "Total Timesteps: 10237 Episode Num: 726 Reward: 6.869718224330331\n",
            "Total Timesteps: 10246 Episode Num: 727 Reward: 5.96664832065732\n",
            "Total Timesteps: 10255 Episode Num: 728 Reward: 7.052352575054649\n",
            "Total Timesteps: 10264 Episode Num: 729 Reward: 7.278061632785329\n",
            "Total Timesteps: 10274 Episode Num: 730 Reward: 7.499327031055634\n",
            "Total Timesteps: 10284 Episode Num: 731 Reward: 7.31539139536718\n",
            "Total Timesteps: 10293 Episode Num: 732 Reward: 7.320355233960206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGTx9WU40Ux2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIBm_ICvGFNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "c55f4850-f306-4b29-f53f-9d915b07301b"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_21 = nn.Linear(300, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = F.relu(self.layer_21(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_21 = nn.Linear(300, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_51 = nn.Linear(300, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = F.relu(self.layer_21(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = F.relu(self.layer_51(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = F.relu(self.layer_21(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"Walker2DBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_Walker2DBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 78.118477\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqI59eaxH4k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}